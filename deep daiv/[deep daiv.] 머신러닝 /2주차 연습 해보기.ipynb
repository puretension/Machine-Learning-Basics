{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "s = Service('/Users/idohyeong/Documents/chromedriver')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "time.sleep(1)\n",
    "\n",
    "url = 'https://www.naver.com/'\n",
    "driver.get(url)\n",
    "\n",
    "query = driver.find_element(by='xpath',value='//*[@id=\"query\"]')\n",
    "query.send_keys('AI')\n",
    "\n",
    "btn = driver.find_element(by='xpath',value='//*[@id=\"sform\"]/fieldset/button')\n",
    "btn.click()\n",
    "\n",
    "news_tab = driver.find_element(by = 'xpath', value = '//*[@id=\"lnb\"]/div[1]/div/ul/li[3]/a')\n",
    "news_tab.click()\n",
    "\n",
    "html = driver.page_source\n",
    "# print(html)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 기사 URL 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 임시 URL 설정\n",
    "tmp_url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=AI&sort=2&photo=0&field=0&pd=3&ds=2023.08.01&de=2023.08.01&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20230801to20230801,a:all&start=1'\n",
    "\n",
    "from urllib.parse import urlparse,parse_qs\n",
    "\n",
    "# 쿼리 파라미터 추출\n",
    "params = parse_qs(urlparse(tmp_url).query) \n",
    "params # dic\n",
    "\n",
    "# 페이지 시작 번호 설정\n",
    "params['start'] = 1 \n",
    "\n",
    "# 오늘과 지난주 날짜 계산\n",
    "from datetime import datetime, timedelta\n",
    "today = datetime.today()\n",
    "lastweek = today - timedelta(days=7)\n",
    "today = today.strftime('%Y.%m.%d')\n",
    "lastweek = lastweek.strftime('%Y.%m.%d')\n",
    "\n",
    "# 날짜 범위 생성\n",
    "import pandas as pd\n",
    "date_list = pd.date_range(lastweek, today) # 지난주~오늘 -> 리스트 변환\n",
    "print(date_list)\n",
    "\n",
    "url = 'https://search.naver.com/search.naver'\n",
    "\n",
    "news_url_list = []\n",
    "\n",
    "for date in date_list:\n",
    "    # 해당 날짜를 문자열 형식으로 변환하여 검색 파라미터를 업데이트\n",
    "    date_string = date.strftime('%Y.%m.%d') # 2023.08.01\n",
    "    params['ds'] = date_string\n",
    "    params['de'] = date_string\n",
    "    params['nso'] = 'so:r,p:from{fr}to{to},a:all'.format(fr = date.strftime('%Y%m%d'), to = date.strftime('%Y%m%d'))\n",
    "    time.sleep(1)\n",
    "\n",
    "# 아래와 같은 형태(ds, de, nso)\n",
    "# https://search.naver.com/search.naver?where=news&sm=tab_pge&query=AI\n",
    "# &sort=2&photo=0&field=0&pd=3&ds=2023.08.01&de=2023.08.01&mynews=0&office_type=0&\n",
    "# office_section_code=0&news_office_checked=&nso=so:r,p:from20230801to20230801,a:all&start=1\n",
    "    \n",
    "    res = requests.get(url, params=params)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # a 태그중에 클래스가 info, text = 네이버뉴스인 모든 요소찾아 리스트로변환(네이버뉴스'로 연결되는 링크들이 포함)\n",
    "    news_url_list_part = soup.find_all('a', 'info', text = '네이버뉴스')\n",
    "    print(date_string, len(news_url_list_part), '개')\n",
    "    news_url_list.extend(news_url_list_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url을 parameter이용하여 가져온후 파싱을 거쳐 soup로 변환 \n",
    "def get_soup(url, params):\n",
    "    res = requests.get(url, params = params)\n",
    "    time.sleep(1)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "#  주어진 BeautifulSoup 객체에서 '네이버뉴스' 링크를 모두 찾아서 반환\n",
    "def get_news_url_list(soup):\n",
    "    news_url_list = soup.find_all('a', 'info', text = '네이버뉴스')\n",
    "    return news_url_list\n",
    "\n",
    "def get_page_list(soup):\n",
    "    # 클래스 이름이 'sc_page_inner'인 <div> 태그를 찾고,\n",
    "    #  <div> 태그 내부의 모든 <a> 태그들을 찾아 각각의 텍스트를 정수로 변환\n",
    "    page_list = list(map(lambda x: int(x.text),soup.find('div', 'sc_page_inner').find_all('a')))\n",
    "    return page_list\n",
    "\n",
    "def get_daily_news(url, params):\n",
    "    news_url_list = []\n",
    "    n = 1\n",
    "\n",
    "    soup = get_soup(url, params)\n",
    "    page_list = get_page_list(soup)\n",
    "\n",
    "    while n in page_list:\n",
    "\n",
    "        soup = get_soup(url, params)\n",
    "        news_url_list_part = get_news_url_list(soup)\n",
    "        news_url_list.extend(news_url_list_part)\n",
    "\n",
    "        time.sleep(1)\n",
    "        page_list = get_page_list(soup)\n",
    "        print(n, page_list)\n",
    "        \n",
    "        n += 1\n",
    "        params['start'] += 10\n",
    "\n",
    "    print(len(news_url_list), '개,', n-1, '페이지')\n",
    "    return news_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 URL 수집 | 하루\n",
    "date = date_list[0]\n",
    "print(date)\n",
    "params['start'] = 1\n",
    "\n",
    "date_string = date.strftime('%Y.%m.%d')\n",
    "params['ds'] = date_string\n",
    "params['de'] = date_string\n",
    "params['nso'] = 'so:r,p:from{fr}to{to}'.format(fr = date.strftime('%Y%m%d'), to = date.strftime('%Y%m%d'))\n",
    "time.sleep(1)\n",
    "\n",
    "# print(date_string)\n",
    "daily_news_list = get_daily_news(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (참고) 뉴스 URL 수집 | 모든 페이지\n",
    "url = 'https://search.naver.com/search.naver'\n",
    "params = parse_qs(urlparse(tmp_url).query)\n",
    "\n",
    "news_url_list = []\n",
    "for date in date_list:\n",
    "    params['start'] = 1\n",
    "\n",
    "    date_string = date.strftime('%Y.%m.%d')\n",
    "    params['ds'] = date_string\n",
    "    params['de'] = date_string\n",
    "    params['nso'] = 'so:r,p:from{fr}to{to}'.format(fr = date.strftime('%Y%m%d'), to = date.strftime('%Y%m%d'))\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    print(date_string)\n",
    "    daily_news_list = get_daily_news(url, params)\n",
    "    news_url_list.extend(daily_news_list)\n",
    "\n",
    "# 1주일 분량에 46분 30초 소요\n",
    "\n",
    "# 수집된 URL 개수 확인\n",
    "len(daily_news_list)\n",
    "\n",
    "# 뉴스 URL 추출\n",
    "news_url_list = list(map(lambda x : x.get('href'), daily_news_list))\n",
    "\n",
    "# for문을 이용하지 않고 뉴스 맵 함수를 이용하여 빠르게 URL을 추출합니다.\n",
    "# URL 데이터 프레임으로 변환\n",
    "df = pd.DataFrame(news_url_list, columns = ['URL']); print(df['URL'][61])\n",
    "\n",
    "# URL 데이터 프레임 저장\n",
    "\n",
    "path = ''\n",
    "file_name = '네이버 뉴스_AI_{}.csv'.format(date_string)\n",
    "df.to_csv(path + file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1da8bb1e470ff509bdae4e915df9bfc787d66424978116436e3017457e3e36c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
