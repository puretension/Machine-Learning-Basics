{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 크롤링하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 삼성전자 검색후 title, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "resp = requests.get(\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90\")\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "links = soup.select(\".news_tit\")\n",
    "# print(links)\n",
    "\n",
    "for link in links:\n",
    "    title = link.text # 태크안에 텍스트요소 가져옴 \n",
    "    url = link.attrs['href'] # href속성 가져옴\n",
    "    print(title, url)\n",
    "\n",
    "# Protocol://Domain/Path?Parameter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검색어 입력받고 1~10 page title, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "num = 1\n",
    "# keyword = input('검색어를 입력하세요>>')\n",
    "keyword = pyautogui.prompt('검색어를 입력하세요.')\n",
    "\n",
    "for i in range(1,101,10):\n",
    "    print(f\"{num}page입니다@@@@@@@@@@@@@@@@@@@@\")\n",
    "    resp = requests.get(f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}&start{i}\")\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    links = soup.select(\".news_tit\")\n",
    "# print(links)\n",
    "    for link in links:\n",
    "        title = link.text # 태크안에 텍스트요소 가져옴 \n",
    "        url = link.attrs['href'] # href속성 가져옴\n",
    "        print(title, url)\n",
    "    num += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 뉴스 본문 크롤링\n",
    "\n",
    "왜 네이버 뉴스만 크롤링?\n",
    "네이버 뉴스의 본문은 전부 생김새가 동일하나 다른 뉴스들은 생김새가 달라 크롤링이 어렵다\n",
    "\n",
    "##### 네이버 뉴스 링크 가져오는 전략\n",
    "1. div.info_group 아래에 a.info가 2개이상인것만\n",
    "2. 2번째 a.info의 href속성값 가져온다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 페이지내에 링크만 10개 가져올때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "\n",
    "resp = requests.get('https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90')\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links = soup.select(\"div.info_group\")\n",
    "\n",
    "for link in links:\n",
    "    a_tags = link.find_all('a', href=True)\n",
    "    if len(a_tags) >= 2:\n",
    "        url = a_tags[1]['href']\n",
    "        print(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 본문 크롤링할때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "resp = requests.get('https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90')\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "articles = soup.select(\"div.info_group\")\n",
    "\n",
    "for article in articles:\n",
    "    links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "    if len(links) >= 2:\n",
    "        url = links[1].attrs['href']\n",
    "        # print(url)\n",
    "        resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 각링크들 받아온후 \n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # print(soup)\n",
    "        contents = soup.select_one('#dic_area') # dic_area: 본문 뉴스 id(#) OR newsct_body: 본문 뉴스 class(.)\n",
    "        if contents:  # 결과가 있을 경우만 출력\n",
    "            print(contents.text)\n",
    "        time.sleep(0.5) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색어 입력받고, 해당 (네이버)뉴스들 출력하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# 사용자로부터 검색어를 입력받습니다.\n",
    "query = input(\"검색어를 입력하세요: \")\n",
    "\n",
    "base_url = 'https://search.naver.com/search.naver'\n",
    "params = {\n",
    "    'where': 'news',\n",
    "    'sm': 'tab_jum',\n",
    "    'query': query\n",
    "}\n",
    "\n",
    "resp = requests.get(base_url, params=params)\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "articles = soup.select(\"div.info_group\")\n",
    "\n",
    "for article in articles:\n",
    "    links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "    if len(links) >= 2:\n",
    "        url = links[1].attrs['href']\n",
    "        resp = requests.get(url, headers={'User-agent': 'Mozilla/5.0'})\n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        print(url)\n",
    "        contents = soup.select_one('#dic_area')\n",
    "        if contents: \n",
    "            print(contents.text)\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연예 뉴스 크롤링하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "resp = requests.get('https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%97%90%EC%8A%A4%ED%8C%8C') # 에스파 검색링크\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "articles = soup.select(\"div.info_group\")\n",
    "\n",
    "for article in articles:\n",
    "    links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "    if len(links) >= 2:\n",
    "        url = links[1].attrs['href']\n",
    "        # print(url)\n",
    "        resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 각링크들 받아온후 \n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        print(url)\n",
    "\n",
    "        # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "        if \"entertain\" in resp.url:\n",
    "            title = soup.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "            contents = soup.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "        else:\n",
    "            title = soup.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "            contents = soup.select_one('#dic_area') # 일반 뉴스 본문\n",
    "        print(\"==========링크===========\\n\",url)\n",
    "        print(\"==========제목===========\\n\",title.text.strip())\n",
    "        print(\"==========본문===========\\n\",contents.text.strip())\n",
    "        time.sleep(0.5) # 필수!!!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스포츠뉴스는 사이트의 생김새(HTML)가 다르기때문에 오류가 난다\n",
    "### 해결법 \n",
    "- 본문 사이트 요청시 사이트 URL이 스포츠뉴스라면? \n",
    "- CSS 선택자를 바꿔준다\n",
    "\n",
    "#### if 연예뉴스 URL \n",
    "     연예뉴스 사이트의 CSS 선택자 사용\n",
    "#### elif 스포츠뉴스 URL\n",
    "     스포츠 뉴스 사이트의 CSS 선택자 사용\n",
    "#### else \n",
    "     일반 뉴스 사이트의 CSS 선택자 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "resp = requests.get('https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query=%EC%98%A4%EC%8A%B9%ED%99%98+&oquery=%EC%86%90%ED%9D%A5%EB%AF%BC&tqi=iL45NlprvTosshov5lVssssstdZ-295557') # 에스파 검색링크\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "articles = soup.select(\"div.info_group\")\n",
    "\n",
    "for article in articles:\n",
    "    links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "    if len(links) >= 2:\n",
    "        url = links[1].attrs['href']\n",
    "        # print(url)\n",
    "        resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 각링크들 받아온후 \n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        print(url)\n",
    "\n",
    "        # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "        if \"entertain\" in resp.url:\n",
    "             title = soup.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "             contents = soup.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "\n",
    "        elif \"sports\" in resp.url:\n",
    "                title = soup.select_one(\"h4.title\")\n",
    "                contents = soup.select_one('#newsEndContents')\n",
    "\n",
    "                    # contents중에서 불필요한 부분(기자명, 다른광고) div 삭제하기\n",
    "                divs = contents.select('div')\n",
    "                for div in divs:\n",
    "                    div.decompose()  # 삭제\n",
    "                paragraphs = contents.select('p')\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "        else:\n",
    "                title = soup.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "                contents = soup.select_one('#dic_area') # 일반 뉴스 본문\n",
    "\n",
    "        print(\"==========링크===========\\n\",url)\n",
    "        print(\"==========제목===========\\n\",title.text.strip())\n",
    "        print(\"==========본문===========\\n\",contents.text.strip())\n",
    "        time.sleep(0.5) # 필수!!!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색어 입력받고 해당 (네이버)뉴스 본문들 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "keyword = pyautogui.prompt('검색어를 입력하세요.')\n",
    "resp = requests.get(\n",
    "    f'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query={keyword}')\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "articles = soup.select(\"div.info_group\")\n",
    "\n",
    "for article in articles:\n",
    "    links = article.select(\"a.info\")  # a tag에 클래스는 info\n",
    "    if len(links) >= 2:\n",
    "        url = links[1].attrs['href']\n",
    "        # print(url)\n",
    "        resp = requests.get(\n",
    "            url, headers={'User-agent': 'Mozila/5.0'})  # 각링크들 받아온후\n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        print(url)\n",
    "        # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "        if \"entertain\" in resp.url:\n",
    "             title = soup.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "             contents = soup.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "\n",
    "        elif \"sports\" in resp.url:\n",
    "                title = soup.select_one(\"h4.title\")\n",
    "                contents = soup.select_one('#newsEndContents')\n",
    "\n",
    "                    # contents중에서 불필요한 부분(기자명, 다른광고) div 삭제하기\n",
    "                divs = contents.select('div')\n",
    "                for div in divs:\n",
    "                    div.decompose()  # 삭제\n",
    "                paragraphs = contents.select('p')\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "        else:\n",
    "                title = soup.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "                contents = soup.select_one('#dic_area') # 일반 뉴스 본문\n",
    "\n",
    "        print(\"==========링크===========\\n\", url)\n",
    "        print(\"==========제목===========\\n\", title.text.strip())\n",
    "        print(\"==========본문===========\\n\", contents.text.strip())\n",
    "        time.sleep(0.5)  # 필수!!!!!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자로부터 페이지 수 입력받고 그 페이지 수만큼 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "\n",
    "keyword = pyautogui.prompt(\"검색어를 입력하세요\")\n",
    "lastpage = int(pyautogui.prompt(\"몇 페이지까지 크롤링 할까요?\")) # int변환 필요\n",
    "\n",
    "page_num = 1\n",
    "for i in range(1, lastpage * 10, 10):\n",
    "    print(f\"{page_num} 페이지 크롤링 중 입니다.======================================\")\n",
    "    resp = requests.get(f'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query={keyword}start={i}')\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select(\"div.info_group\")\n",
    "\n",
    "    for article in articles:\n",
    "        links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "        if len(links) >= 2:\n",
    "            url = links[1].attrs['href']\n",
    "            # print(url)\n",
    "            resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 각링크들 받아온후 \n",
    "            html = resp.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            print(url)\n",
    "\n",
    "                       # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "            if \"entertain\" in resp.url:\n",
    "                title = soup.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "                contents = soup.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "\n",
    "            elif \"sports\" in resp.url:\n",
    "                title = soup.select_one(\"h4.title\")   \n",
    "                contents = soup.select_one('#newsEndContents')\n",
    "\n",
    "                    # contents중에서 불필요한 부분(기자명, 다른광고) div 삭제하기 \n",
    "                divs = contents.select('div')\n",
    "                for div in divs:\n",
    "                    div.decompose() # 삭제\n",
    "                paragraphs = contents.select('p')\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "            else:\n",
    "                title = soup.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "                contents = soup.select_one('#dic_area') # 일반 뉴스 본문\n",
    "\n",
    "            print(\"==========링크===========\\n\",url)\n",
    "            print(\"==========제목===========\\n\",title.text.strip())\n",
    "            print(\"==========본문===========\\n\",contents.text.strip())\n",
    "            time.sleep(0.5) # 필수!!!!!\n",
    "    page_num += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드 문서 다루기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "#1. 워드 생성하기 \n",
    "document = Document()\n",
    "\n",
    "#2. 워드 데이터 추가하기 \n",
    "document.add_heading('기사제목', level=0)\n",
    "document.add_paragraph('기사 링크')\n",
    "document.add_paragraph('기사 본문')\n",
    "\n",
    "#3. 워드 저장하기\n",
    "document.save(\"text.docx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크롤링 결과를 워드파일에 저장해 보자!\n",
    "파일 생성 위치, 데이터 입력 위치, 파일 저장 위치 고려하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지 크롤링 중...\n",
      "2 페이지 크롤링 중...\n",
      "3 페이지 크롤링 중...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "from docx import Document\n",
    "\n",
    "document = Document()\n",
    "\n",
    "keyword = pyautogui.prompt(\"검색어를 입력하세요\")\n",
    "lastpage = int(pyautogui.prompt(\"몇 페이지까지 크롤링 할까요?\")) # int변환 필요\n",
    "\n",
    "page_num = 1\n",
    "for i in range(1, lastpage * 10, 10):\n",
    "    print(f\"{page_num} 페이지 크롤링 중...\")\n",
    "    resp = requests.get(f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}start={i}')\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select(\"div.info_group\")\n",
    "\n",
    "    for article in articles:\n",
    "        links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "        if len(links) >= 2:\n",
    "            url = links[1].attrs['href']\n",
    "            # print(url)\n",
    "            resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 각링크들 받아온후 \n",
    "            html = resp.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "            if \"entertain\" in resp.url:\n",
    "                title = soup.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "                contents = soup.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "\n",
    "            elif \"sports\" in resp.url:\n",
    "                title = soup.select_one(\"h4.title\")   \n",
    "                contents = soup.select_one('#newsEndContents')\n",
    "\n",
    "                    # contents중에서 불필요한 부분(기자명, 다른광고) div 삭제하기 \n",
    "                divs = contents.select('div')\n",
    "                for div in divs:\n",
    "                    div.decompose() # 삭제\n",
    "                paragraphs = contents.select('p')\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "            else:\n",
    "                title = soup.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "                contents = soup.select_one('#dic_area') # 일반 뉴스 본문\n",
    "\n",
    "            document.add_heading(title.text.strip(), level=0)\n",
    "            document.add_paragraph(url)\n",
    "            document.add_paragraph(contents.text.strip())\n",
    "            time.sleep(0.5) # 필수!!!!!\n",
    "    page_num += 1\n",
    "document.save(f\"{keyword}_result.docx\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드 다루기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "# 엑샐 생성하기 \n",
    "wb = Workbook()\n",
    "\n",
    "# 엑셀 시트 생성하기\n",
    "ws = wb.create_sheet(\"이도형\")\n",
    "ws['A1'] = '이도형'\n",
    "\n",
    "wb.save('DHL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지 크롤링 중...\n",
      "2 페이지 크롤링 중...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Alignment\n",
    "import time\n",
    "\n",
    "keyword = pyautogui.prompt(\"검색어를 입력하세요\")\n",
    "lastpage = int(pyautogui.prompt(\"몇 페이지까지 크롤링 할까요?\")) # int변환 필요\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.create_sheet(keyword)\n",
    "# 너비 설정 \n",
    "ws.column_dimensions['A'].width = 60\n",
    "ws.column_dimensions['B'].width = 60\n",
    "ws.column_dimensions['C'].width = 120\n",
    "\n",
    "page_num = 1\n",
    "row = 1\n",
    "\n",
    "for i in range(1, lastpage * 10, 10):\n",
    "    print(f\"{page_num} 페이지 크롤링 중...\")\n",
    "    resp = requests.get(f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}&start={i}\")\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select(\"div.info_group\")\n",
    "\n",
    "    for article in articles:\n",
    "        links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "        if len(links) >= 2:\n",
    "            url = links[1].attrs['href']\n",
    "            # print(url)\n",
    "            resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 에러 방지\n",
    "            html = resp.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "            if \"entertain\" in resp.url:\n",
    "                title = soup.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "                contents = soup.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "\n",
    "            elif \"sports\" in resp.url:\n",
    "                title = soup.select_one(\"h4.title\")   \n",
    "                contents = soup.select_one('#newsEndContents')\n",
    "\n",
    "                    # contents중에서 불필요한 부분(기자명, 다른광고) div 삭제하기 \n",
    "                divs = contents.select('div')\n",
    "                for div in divs:\n",
    "                    div.decompose() # 삭제\n",
    "                paragraphs = contents.select('p')\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "            else:\n",
    "                title = soup.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "                contents = soup.select_one('#dic_area') # 일반 뉴스 본문\n",
    "\n",
    "            ws[f'A{row}'] = url\n",
    "            ws[f'B{row}'] = title.text.strip()\n",
    "            ws[f'C{row}'] = contents.text.strip()\n",
    "            ws[f\"C{row}\"].alignment = Alignment(wrap_text=True)  \n",
    "            row += 1\n",
    "            time.sleep(0.4)\n",
    "    page_num += 1\n",
    "    wb.save(f\"{keyword}_result.xlsx\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막 페이지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지 크롤링 중...\n",
      "2 페이지 크롤링 중...\n",
      "3 페이지 크롤링 중...\n",
      "마지막 페이지입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyautogui\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Alignment\n",
    "import time\n",
    "\n",
    "keyword = pyautogui.prompt(\"검색어를 입력하세요\")\n",
    "lastpage = int(pyautogui.prompt(\"몇 페이지까지 크롤링 할까요?\")) # int변환 필요\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.create_sheet(keyword)\n",
    "# 너비 설정 \n",
    "ws.column_dimensions['A'].width = 60\n",
    "ws.column_dimensions['B'].width = 60\n",
    "ws.column_dimensions['C'].width = 120\n",
    "\n",
    "page_num = 1\n",
    "row = 1\n",
    "\n",
    "for i in range(1, lastpage * 10, 10):\n",
    "    print(f\"{page_num} 페이지 크롤링 중...\")\n",
    "    resp = requests.get(f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}&start={i}\")\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select(\"div.info_group\")\n",
    "\n",
    "    for article in articles:\n",
    "        links = article.select(\"a.info\") # a tag에 클래스는 info\n",
    "        if len(links) >= 2:\n",
    "            url = links[1].attrs['href']\n",
    "            # print(url)\n",
    "            resp = requests.get(url, headers={'User-agent': 'Mozila/5.0'}) # 에러 방지\n",
    "            html = resp.text\n",
    "            soup_sub = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # 만약 연예 뉴스라면? redirection이 발생하므로 url이 아닌 resp.url\n",
    "            if \"entertain\" in resp.url:\n",
    "                title = soup_sub.select_one(\".end_tit\") # 연예 뉴스 타이틀 \n",
    "                contents = soup_sub.select_one(\"#articeBody\") # 연예 뉴스 본문\n",
    "\n",
    "            elif \"sports\" in resp.url:\n",
    "                title = soup_sub.select_one(\"h4.title\")   \n",
    "                contents = soup_sub.select_one('#newsEndContents')\n",
    "\n",
    "                    # contents중에서 불필요한 부분(기자명, 다른광고) div 삭제하기 \n",
    "                divs = contents.select('div')\n",
    "                for div in divs:\n",
    "                    div.decompose() # 삭제\n",
    "                paragraphs = contents.select('p')\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "            else:\n",
    "                title = soup_sub.select_one(\".media_end_head_title\") # 일반 뉴스 타이틀 \n",
    "                contents = soup_sub.select_one('#dic_area') # 일반 뉴스 본문\n",
    "\n",
    "            ws[f'A{row}'] = url\n",
    "            ws[f'B{row}'] = title.text.strip()\n",
    "            ws[f'C{row}'] = contents.text.strip()\n",
    "            ws[f\"C{row}\"].alignment = Alignment(wrap_text=True)  \n",
    "            row += 1\n",
    "            time.sleep(0.4)\n",
    "\n",
    "    isLastPage = soup.select_one('a.btn_next').attrs['aria-disabled']\n",
    "    if isLastPage == 'true':\n",
    "        print(\"마지막 페이지입니다.\")\n",
    "        break \n",
    "    page_num += 1\n",
    "    wb.save(f\"{keyword}_result.xlsx\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 클라우드\n",
    "#### 중요한 단어를 더 잘보이게 하는방법\n",
    "- 본문 내용 합치기 -> 워드클라우드 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pyautogui\n",
    "import pyperclip\n",
    "\n",
    "keyword = pyautogui.prompt(\"검색어를 입력하세요\")\n",
    "lastpage = int(pyautogui.prompt(\"몇 페이지까지 크롤링 할까요?\")) # int변환 필요\n",
    "\n",
    "page_num = 1\n",
    "total_content = \"\"\n",
    "article_num = 0\n",
    "\n",
    "for i in range(1, lastpage * 10, 10):\n",
    "    print(f\"{page_num} page...\")\n",
    "    response = requests.get(f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}&start={i}\")\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    articles = soup.select(\"div.info_group\")\n",
    "\n",
    "    for article in articles:\n",
    "        links = article.select(\"a.info\")\n",
    "        if len(links) >= 2:\n",
    "            url = links[1].attrs[\"href\"]\n",
    "            response = requests.get(url, headers={'User-agent': 'Mozila/5.0'})    \n",
    "            html = response.text                                                 \n",
    "            soup_sub = BeautifulSoup(html, \"html.parser\")                         \n",
    "\n",
    "            # separation\n",
    "            if \"entertain\" in response.url:                                     \n",
    "                content = soup_sub.select_one(\"#articeBody\")                        \n",
    "            elif \"sports\" in response.url:                                        \n",
    "                content = soup_sub.select_one(\"#newsEndContents\")             \n",
    "\n",
    "                # delete unnecessary elements\n",
    "                divs = content.select(\"div\")\n",
    "                for div in divs:\n",
    "                    div.decompose()\n",
    "\n",
    "                paragraphs = content.select(\"p\")\n",
    "                for p in paragraphs:\n",
    "                    p.decompose()\n",
    "\n",
    "            else:\n",
    "                content = soup_sub.select_one(\"#dic_area\")                         \n",
    "            print(\"========BODY========\\n\", content.text.strip())\n",
    "            total_content += content.text.strip()\n",
    "            article_num += 1\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    is_last_page = soup.select_one(\"a.btn_next\").attrs[\"aria-disabled\"]\n",
    "    if is_last_page == \"true\":\n",
    "        print(\"last page\")\n",
    "        break\n",
    "\n",
    "    page_num += 1\n",
    "\n",
    "print(f\"{article_num} articles 크롤링 완료\")\n",
    "pyperclip.copy(total_content)\n",
    "pyautogui.alert('클립보드에 복사되었습니다.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1da8bb1e470ff509bdae4e915df9bfc787d66424978116436e3017457e3e36c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
